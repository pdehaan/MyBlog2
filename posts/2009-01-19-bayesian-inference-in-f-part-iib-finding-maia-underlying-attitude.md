---
id: 313
title: 'Bayesian inference in F# – Part IIb – Finding Maia underlying attitude'
date: 2009-01-19T11:48:00+00:00
author: lucabol
layout: post
guid: https://blogs.msdn.microsoft.com/lucabol/2009/01/19/bayesian-inference-in-f-part-iib-finding-maia-underlying-attitude/
orig_url:
  - http://blogs.msdn.microsoft.com/b/lucabol/archive/2009/01/19/bayesian-inference-in-f-part-iib-finding-maia-underlying-attitude.aspx
orig_site_id:
  - "3896"
orig_post_id:
  - "9340323"
orig_parent_id:
  - "9340323"
orig_thread_id:
  - "630418"
orig_application_key:
  - lucabol
orig_post_author_id:
  - "3896"
orig_post_author_username:
  - lucabol
orig_post_author_created:
  - 'Apr  2 2005 10:57:56:453AM'
orig_is_approved:
  - "1"
orig_attachment_count:
  - "0"
orig_url_title:
  - http://blogs.msdn.com/b/lucabol/archive/2009/01/19/bayesian-inference-in-f-part-iib-finding-maia-underlying-attitude.aspx
opengraph_tags:
  - |
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Bayesian inference in F# &ndash; Part IIb &ndash; Finding Maia underlying attitude" />
    <meta property="og:url" content="https://blogs.msdn.microsoft.com/lucabol/2009/01/19/bayesian-inference-in-f-part-iib-finding-maia-underlying-attitude/" />
    <meta property="og:site_name" content="Luca Bolognese&#039;s WebLog" />
    <meta property="og:description" content="Other parts: Part I – Background Part II – A simple example – modeling Maia The previous post ended on this note.let MaiaJointProb attitude action = match attitude with | Happy -&gt; happyActions |&gt; List.assoc action | UnHappy -&gt; unHappyActions |&gt; List.assoc action | Quiet -&gt; quietActions |&gt; List.assoc action This is just a two..." />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Bayesian inference in F# &ndash; Part IIb &ndash; Finding Maia underlying attitude" />
    <meta name="twitter:url" content="https://blogs.msdn.microsoft.com/lucabol/2009/01/19/bayesian-inference-in-f-part-iib-finding-maia-underlying-attitude/" />
    <meta name="twitter:description" content="Other parts: Part I – Background Part II – A simple example – modeling Maia The previous post ended on this note.let MaiaJointProb attitude action = match attitude with | Happy -&gt; happyActions |&gt; List.assoc action | UnHappy -&gt; unHappyActions |&gt; List.assoc action | Quiet -&gt; quietActions |&gt; List.assoc action This is just a two..." />
    
restapi_import_id:
  - 5c011e0505e67
categories:
  - Uncategorized
tags:
  - fsharp
  - Statistics
---
Other parts:

  * [Part I – Background](http://blogs.msdn.com/lucabol/archive/2008/11/07/bayesian-inference-in-f-part-i-background.aspx) 
  * [Part II – A simple example – modeling Maia](http://blogs.msdn.com/lucabol/archive/2008/11/26/bayesian-inference-in-f-part-iia-a-simple-example-modeling-maia.aspx) 

The previous post ended on this note.

<pre class="code"><span style="color:blue;">let </span>MaiaJointProb attitude action =
    <span style="color:blue;">match </span>attitude <span style="color:blue;">with
    </span>| Happy     <span style="color:blue;">-&gt; </span>happyActions |&gt; List.assoc action
    | UnHappy   <span style="color:blue;">-&gt; </span>unHappyActions |&gt; List.assoc action
    | Quiet     <span style="color:blue;">-&gt; </span>quietActions |&gt; List.assoc action</pre>

This is just a two by two matrix. It simply represents which probability is associated to an (attitude, action) tuple. It is useful to think about it in these terms, because it makes easier to grasp the following function:

<pre class="code"><span style="color:green;">/// Conditional probability of a mental state, given a particular observed action
</span><span style="color:blue;">let </span>MaiaLikelihood action = <span style="color:blue;">fun </span>attitude <span style="color:blue;">-&gt; </span>MaiaJointProb attitude action</pre>



This is simply a row in the matrix. It answers the question: given that I observe a particular action, what is the probability that Maia has a certain attitude?. This is called “likelihood function” in statistics. Its general form is: given that a I observe an outcome, what is the probability that it is generated by a process with a particular parameter?

A related question is then: what if I observe a sequence of independent actions? What is the probability that the baby has a certain attitude then? This is answered by the following:

<pre class="code"><span style="color:green;">/// Multiple applications of the previous conditional probabilities for a series of actions (multiplied)
</span><span style="color:blue;">let </span>MaiaLikelihoods actions =
    <span style="color:blue;">let </span>composeLikelihoods previousLikelihood action  = <span style="color:blue;">fun </span>attitude <span style="color:blue;">-&gt; </span>previousLikelihood attitude * MaiaLikelihood action attitude
    actions |&gt; Seq.fold composeLikelihoods (<span style="color:blue;">fun </span>attitude <span style="color:blue;">-&gt; </span>1.)</pre>

It is a trivial extension of the previous function (really), once you know that to combine likelihoods you multiply them.

We now need to describe what our prior is. A prior is our preconceived notion about a particular parameter (in this case the baby’s attitude). You might be tempted to express that notion with a single value, but that would be inaccurate. You need to indicate how confident you are about it. In statistics you do that by choosing a distribution for your belief. This is one of the beauties of Bayesian statistics, everything is a probability distribution. In this case, we really don’t have any previous belief, so we pick the uniform distribution.

<pre class="code"><span style="color:blue;">let </span>MaiaUniformPrior attitude = 1. / 3.</pre>

Think of this as: you haven’t read any baby-attitude-specific study or received any external information about the likely attitude of Maia, so you cannot prefer one attitude over another.

We are almost done. Now we have to apply the Bayesian theorem and get the un-normalized posterior distribution. Forget about the un-normalized word. What is a posterior distribution? This is your output, your return value. It says: given my prior belief on the value of a parameter and given the outcomes that I observed, this is what I now believe the parameter to be. In this case it goes like: I had no opinion on Maia’s attitude to start with, but after I observed her behavior for a while, I now think she is Happy with probability X, UnHappy with probability Y and Quiet with probability Z.

<pre class="code"><span style="color:green;">/// Calculates the unNormalized posterior given prior and likelihood
</span><span style="color:blue;">let </span>unNormalizedPosterior (prior:'a <span style="color:blue;">-&gt; </span>float) likelihood =
    <span style="color:blue;">fun </span>theta <span style="color:blue;">-&gt; </span>prior theta * likelihood theta</pre>



We then need to normalize this thing (it doesn’t sum to one). The way to do it is to divide each probability by the sum of the probabilities for all the possible outcomes.

<pre class="code"><span style="color:green;">/// All possible values for the unobservable parameter (mental state)
</span><span style="color:blue;">let </span>support = [Happy; UnHappy; Quiet]
<span style="color:green;">/// Normalize the posterior (it integrates to 1.)
</span><span style="color:blue;">let </span>posterior prior likelihood =
    <span style="color:blue;">let </span>post = unNormalizedPosterior prior likelihood
    <span style="color:blue;">let </span>sum = support |&gt; List.sum_by (<span style="color:blue;">fun </span>attitude <span style="color:blue;">-&gt; </span>post attitude)
    <span style="color:blue;">fun </span>attitude <span style="color:blue;">-&gt; </span>post attitude / sum</pre>

We are done. Now we can now start modeling scenarios. Let’s say that you observe [Smile;Smile;Cry;Smile;LookSilly]. What could the underlying attitude of Maia be?

<pre class="code"><span style="color:blue;">let </span>maiaIsANormalBaby = posterior MaiaUniformPrior (MaiaLikelihoods [Smile;Smile;Cry;Smile;LookSilly])</pre>

We can then execute our little model:

<pre class="code">maiaIsANormalBaby Happy
maiaIsANormalBaby UnHappy
maiaIsANormalBaby Quiet</pre>

And we get (0.5625, 0.0625, 0.375). So Maia is likely to be happy and unlikely to be unhappy. Let’s now model one extreme case:

<pre class="code"><span style="color:green;">/// Extreme cases
</span><span style="color:blue;">let </span>maiaIsLikelyHappyDist = posterior MaiaUniformPrior (MaiaLikelihoods [Smile;Smile;Smile;Smile;Smile;Smile;Smile])
maiaIsLikelyHappyDist Happy
maiaIsLikelyHappyDist UnHappy<br />maiaIsLikelyHappyDist Quiet</pre>

And we get (0.944, 0.000431, 0.05). Now Maia is almost certainly Happy. Notice that I can confidently make this affirmation because my end result is exactly what I was looking for when I started my quest. Using classical statistics, that wouldn’t be the case.

A related question I might want to ask is: given the posterior distribution for attitude that I just found, what is the probability of observing a particular action? In other words, given the model that I built, what does it predict?

<pre class="code"><span style="color:blue;">let </span>posteriorPredictive jointProb posterior =
    <span style="color:blue;">let </span>composeProbs previousProbs attitude = <span style="color:blue;">fun </span>action <span style="color:blue;">-&gt; </span>previousProbs action + jointProb attitude action * posterior attitude
    support |&gt; Seq.fold composeProbs (<span style="color:blue;">fun </span>action <span style="color:blue;">-&gt; </span>0.)
<span style="color:blue;">let </span>nextLikelyUnknownActionDist = posteriorPredictive MaiaJointProb maiaIsLikelyHappyDist</pre>

I don’t have the strength right now to explain the mathematical underpinning of this. In words, this says: considering that Maia can have one of the possible three Attitudes with the probability calculated above, what is the probability that I observe a particular action? Notice that the signature for it is: (Action –> float), which is the compiler way to say it.

Now we can run the thing.

<pre class="code">nextLikelyUnknownActionDist Smile
nextLikelyUnknownActionDist Cry
nextLikelyUnknownActionDist LookSilly</pre>

And we get (0.588, 0.2056, 0.2055). Why is that? We’ll talk about it in the next post.